{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba451e0-46fa-477d-bde9-f4b9109932e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold\n",
    "import gc\n",
    "# !pip install ipywidgets\n",
    "from tqdm.auto import tqdm\n",
    "import Levenshtein\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9e8bda9-170a-44bd-ab3f-02a0c494e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True #False for submission\n",
    "N = -1 #-1 for all samples\n",
    "MODEL_PATH=['model/aslfr-fp16-192d-17l-ctcattjoint-seed42-foldall-last.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e316079-a0c3-4081-82b8-7327a2d5729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf933e28-a0b7-429f-bf5f-b43c633ebb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('character_to_prediction_index.json') as json_file:\n",
    "    CHAR_TO_NUM = json.load(json_file)\n",
    "NUM_TO_CHAR = dict([(y+1,x) for x,y in CHAR_TO_NUM.items()] )\n",
    "NUM_TO_CHAR[60] = 'S'\n",
    "NUM_TO_CHAR[61] = 'E'\n",
    "NUM_TO_CHAR[0] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad135cdc-c742-4bcd-b8bc-260d61b2a56f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '=': 27,\n",
       " '?': 28,\n",
       " '@': 29,\n",
       " '[': 30,\n",
       " '_': 31,\n",
       " 'a': 32,\n",
       " 'b': 33,\n",
       " 'c': 34,\n",
       " 'd': 35,\n",
       " 'e': 36,\n",
       " 'f': 37,\n",
       " 'g': 38,\n",
       " 'h': 39,\n",
       " 'i': 40,\n",
       " 'j': 41,\n",
       " 'k': 42,\n",
       " 'l': 43,\n",
       " 'm': 44,\n",
       " 'n': 45,\n",
       " 'o': 46,\n",
       " 'p': 47,\n",
       " 'q': 48,\n",
       " 'r': 49,\n",
       " 's': 50,\n",
       " 't': 51,\n",
       " 'u': 52,\n",
       " 'v': 53,\n",
       " 'w': 54,\n",
       " 'x': 55,\n",
       " 'y': 56,\n",
       " 'z': 57,\n",
       " '~': 58}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14143ba2-7470-4bc9-a4d8-f7e809fea830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "#for the lip_lr function. LEFT[i] is matching with RIGHT[i](i.e LEFT[i](x) == -RIGHT[i](x)).\n",
    "#computed from https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj\n",
    "\n",
    "LEFT = [\n",
    "         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
    "         265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
    "         282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
    "         299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
    "         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332,\n",
    "         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
    "         367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n",
    "         384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
    "         401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
    "         418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
    "         435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
    "         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,  #LFACE\n",
    "         468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, #LHAND\n",
    "         493, 494, 495, 497, 499, 501, 503, 505, 507, 509, 511, 513, #LPOSE\n",
    "         515, 517, 519, 521, #LLEG\n",
    "         ]\n",
    "\n",
    "RIGHT = [\n",
    "         3, 7, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
    "         39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
    "         60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n",
    "         81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102,\n",
    "         103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
    "         121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
    "         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158,\n",
    "         159, 160, 161, 162, 163, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179,\n",
    "         180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 201,\n",
    "         202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
    "         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
    "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, #RFACE\n",
    "        522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, #RHAND\n",
    "        490, 491, 492, 496, 498, 500, 502, 504, 506, 508, 510, 512, #RPOSE\n",
    "        514, 516, 518, 520, #RLEG\n",
    "        ]\n",
    "\n",
    "CENTRE = [\n",
    "          0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 94, 151, 152, 164, 168, 175, 195, 197, 199, 200, #FACE\n",
    "          489, #POSE\n",
    "          ]\n",
    "\n",
    "print(len(LEFT+RIGHT+CENTRE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44eb9a6-c142-4fa3-8b83-f8c0279518e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n",
      "1629\n"
     ]
    }
   ],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "MAX_LEN = 768\n",
    "CROP_LEN = MAX_LEN\n",
    "NUM_CLASSES  = len(NUM_TO_CHAR.values()) #62\n",
    "PAD = -100.\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "POINT_LANDMARKS = list(range(543))\n",
    "\n",
    "NUM_NODES = len(POINT_LANDMARKS)\n",
    "CHANNELS = 3*NUM_NODES\n",
    "\n",
    "print(NUM_NODES)\n",
    "print(CHANNELS)\n",
    "\n",
    "def interp1d_(x, target_len, method='random'):\n",
    "    length = tf.shape(x)[1]\n",
    "    target_len = tf.maximum(1,target_len)\n",
    "    if method == 'random':\n",
    "        if tf.random.uniform(()) < 0.33:\n",
    "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
    "        else:\n",
    "            if tf.random.uniform(()) < 0.5:\n",
    "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
    "            else:\n",
    "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
    "    else:\n",
    "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
    "    return x\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n",
    "    print(\"INSIDE FILTER\",x.shape)\n",
    "    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n",
    "    x = tf.boolean_mask(x, mask, axis=0)\n",
    "    return x\n",
    "\n",
    "def is_left_handed(x, left=LHAND, right=RHAND):\n",
    "    lhand = tf.gather(x, left, axis=1)\n",
    "    rhand = tf.gather(x, right, axis=1)\n",
    "    lhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(lhand), tf.int32))\n",
    "    rhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(rhand), tf.int32))\n",
    "    return lhand_nans < rhand_nans\n",
    "\n",
    "def flip_lr(x):\n",
    "    x,y,z = tf.unstack(x, axis=-1)\n",
    "    x = 1-x\n",
    "    new_x = tf.stack([x,y,z], -1)\n",
    "    new_x = tf.transpose(new_x, [1,0,2])\n",
    "    l_x = tf.gather(new_x, LEFT, axis=0)\n",
    "    r_x = tf.gather(new_x, RIGHT, axis=0)\n",
    "    c_x = tf.gather(new_x, CENTRE, axis=0)\n",
    "#     new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(left)[...,None], r_x) <-weird behavior in tflite!!!:(\n",
    "#     new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(right)[...,None], l_x)\n",
    "    new_xr = tf.scatter_nd(tf.constant(LEFT)[...,None], r_x, tf.shape(new_x))\n",
    "    new_xl = tf.scatter_nd(tf.constant(RIGHT)[...,None], l_x, tf.shape(new_x))\n",
    "    new_xc = tf.scatter_nd(tf.constant(CENTRE)[...,None], c_x, tf.shape(new_x))\n",
    "    new_x = new_xr + new_xl + new_xc\n",
    "    new_x = tf.transpose(new_x, [1,0,2])\n",
    "    return new_x\n",
    "\n",
    "class Preprocess(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # if tf.rank(inputs) == 3:\n",
    "        #     x = inputs[None,...]\n",
    "        # else:\n",
    "        #     x = inputs\n",
    "        x = inputs\n",
    "#         print(inputs)\n",
    "#         print(inputs.shape)\n",
    "        print(inputs)\n",
    "        print(\"\\n----\\n\",inputs.shape)\n",
    "        x = filter_nans_tf(x)\n",
    "        x = tf.cond(is_left_handed(x), lambda:flip_lr(x), lambda:x)\n",
    "        x = x[None,...]\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "\n",
    "        mean = tf_nan_mean(tf.gather(x, self.point_landmarks, axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant([0.5,0.5,0.],x.dtype), mean)\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "\n",
    "        x = (x - mean)/std\n",
    "\n",
    "        x = tf.concat([\n",
    "            tf.reshape(x, (-1,length,3*len(self.point_landmarks))),\n",
    "            # tf.reshape(dx, (-1,length,3*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "\n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509a27ed-b82c-44f0-807d-5e7ea6dbd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "class MaskingConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, groups=1, strides=1,\n",
    "        dilation_rate=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='glorot_uniform',**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert padding == 'same'\n",
    "        self.filters = filter_dataset_eager_fallback\n",
    "        self.strides = strides\n",
    "        self.groups = groups\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.padding = padding\n",
    "        self.conv = tf.keras.layers.Conv1D(\n",
    "                            filters,\n",
    "                            kernel_size,\n",
    "                            strides=strides,\n",
    "                            groups=groups,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding=padding,\n",
    "                            use_bias=use_bias,\n",
    "                            kernel_initializer=kernel_initializer)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "      if mask is not None:\n",
    "        if self.strides > 1:\n",
    "          mask = mask[:,::self.strides]\n",
    "      return mask\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        if mask is not None:\n",
    "            x = tf.where(mask[...,None], x, tf.constant(0., dtype=x.dtype))\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class MaskingDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size, strides=1,\n",
    "        dilation_rate=1,\n",
    "        padding='same',\n",
    "        use_bias=False,\n",
    "        kernel_initializer='glorot_uniform',**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert padding == 'same'\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.use_bias = use_bias\n",
    "        self.padding = padding\n",
    "        self.conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=strides,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding=padding,\n",
    "                            use_bias=use_bias,\n",
    "                            kernel_initializer=kernel_initializer)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "      if mask is not None:\n",
    "        if self.strides > 1:\n",
    "          mask = mask[:,::self.strides]\n",
    "      return mask\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        if mask is not None:\n",
    "            x = tf.where(mask[...,None], x, tf.constant(0., dtype=x.dtype))\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          strides=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "        \n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'pre_bn')(inputs)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(x)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = MaskingDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            strides=strides,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "        # x = MaskingConv1D(channels_expand,kernel_size,\n",
    "        #     dilation_rate=dilation_rate,\n",
    "        #     groups=8,\n",
    "        #     use_bias=False,\n",
    "        #     name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'conv_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size) and (strides == 1):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def Conv2DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          strides=1,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='gelu',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        # x = MaskingDWConv2D(kernel_size,\n",
    "        #     strides=strides,\n",
    "        #     dilation_rate=dilation_rate,\n",
    "        #     use_bias=False,\n",
    "        #     name=name + '_dwconv')(x)\n",
    "        x = MaskingConv2D(\n",
    "            channels_expand,\n",
    "            kernel_size,\n",
    "            strides=strides,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            groups=channels_expand,\n",
    "            name=name+'_dwconv',\n",
    "        )(x)\n",
    "\n",
    "        # x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn', fused=False)(x)\n",
    "        # x = MaskingBatchNorm2D()(x)\n",
    "        # x = ReduceMask()(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "\n",
    "        # x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size) and (strides == 1):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0028251b-4e00-46d2-97ca-fb97394f95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        # self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.q = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.k = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.v = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_causal_mask(self, q, k):\n",
    "        q_len = tf.shape(q)[1]\n",
    "        k_len = tf.shape(k)[1]\n",
    "        i = tf.range(q_len)[:, None]\n",
    "        j = tf.range(k_len)\n",
    "        mask = i >= j\n",
    "        mask = tf.reshape(mask, (q_len, k_len))\n",
    "        return mask\n",
    "\n",
    "    def merge_input_state(self, input, state, layer):\n",
    "        if input is not None and state is not None:\n",
    "            return tf.keras.layers.Concatenate(axis=1)([state, layer(input)])\n",
    "        elif input is not None and state is None:\n",
    "            return layer(input)\n",
    "        elif input is None and state is not None:\n",
    "            return state\n",
    "        else:\n",
    "            raise ValueError\n",
    "        # return out\n",
    "\n",
    "    def call(self, q, k=None, v=None, key_state=None, value_state=None, return_states=False, use_causal_mask=False):\n",
    "        q = self.q(q)\n",
    "        k = self.merge_input_state(k, key_state, self.k)\n",
    "        v = self.merge_input_state(v, value_state, self.v)\n",
    "        mask = getattr(k, '_keras_mask', None)\n",
    "        if mask is not None:\n",
    "            mask = mask[:,None,None,:]\n",
    "        if use_causal_mask:\n",
    "            if mask is not None:\n",
    "                mask = tf.logical_and(mask, self.get_causal_mask(q,k)[None,None,:,:])\n",
    "            else:\n",
    "                mask = self.get_causal_mask(q,k)[None,None,:,:]\n",
    "        q_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(q))\n",
    "        k_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(k))\n",
    "        v_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(v))\n",
    "        attn = tf.matmul(q_, k_, transpose_b=True) * self.scale\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v_\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        if return_states:\n",
    "            return x, k, v\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class PosEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=64, max_len=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=dim)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, x, positions=None):\n",
    "        if positions is None:\n",
    "            maxlen = tf.shape(x)[1]\n",
    "            positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "    \n",
    "def TransformerDecoderBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish', name=''):\n",
    "    def apply(q,k,v):\n",
    "        x = q\n",
    "        # key_mask=None\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n",
    "        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_self_attn')(x,x,x,use_causal_mask=True)\n",
    "        # print(x.shape, q.shape)\n",
    "        # x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add(name=name + '_add1')([q, x])\n",
    "        attn_out1 = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn2')(x)\n",
    "        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_cross_attn')(x,k,v)\n",
    "        # x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add(name=name + '_add2')([attn_out1, x])\n",
    "        attn_out2 = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn3')(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n",
    "        # x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add(name=name + '_add3')([attn_out2, x])\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc74bebd-9f71-46ef-aeeb-7d368c5d5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "            # print('selfattn mask', mask.shape)\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6ba3b9-e5e4-429d-ac68-e9096f45970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 768, 1629)]          0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, 384, 192)             5565377   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " att_decoder (Functional)    (None, None, 62)             480830    ['input_3[0][0]',             \n",
      "                                                                     'encoder[0][0]']             \n",
      "                                                                                                  \n",
      " ctc_decoder (Functional)    (None, 384, 62)              320318    ['encoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6366525 (24.29 MB)\n",
      "Trainable params: 6336957 (24.17 MB)\n",
      "Non-trainable params: 29568 (115.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(max_len=MAX_LEN, target_len=64, dim=192, dtype='float32'):\n",
    "    ################# ENCODER #################\n",
    "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
    "#     x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp1)\n",
    "    x = inp1\n",
    "    ksize = 17\n",
    "    drop_rate = 0.2\n",
    "    x = tf.keras.layers.Dense(dim,use_bias=False,name='stem_conv')(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=0,strides=2)(x) #drop_rate=0 since we don't want to drop the whole output\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "\n",
    "    encoder = tf.keras.Model(inp1,x,name='encoder')\n",
    "\n",
    "    ################# CTC DECDODER #################\n",
    "    inp3 = tf.keras.Input((x.shape[1],dim),name='ctc_decoder_inp2',dtype=dtype)\n",
    "    x = inp3\n",
    "    x = tf.keras.layers.RNN(tf.keras.layers.GRUCell(dim), return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dense(dim*2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='ctc_classifier')(x) #include sos, eos token\n",
    "    ctc_decoder = tf.keras.Model(inp3,x,name='ctc_decoder')\n",
    "\n",
    "    ################# ATT DECODER #################\n",
    "    inp2 = tf.keras.Input((None,),name='att_decoder_inp1',dtype='int32')\n",
    "    inp3 = tf.keras.Input((x.shape[1],dim),name='att_decoder_inp2',dtype=dtype)\n",
    "\n",
    "    x = inp3\n",
    "#     y = tf.keras.layers.Masking(mask_value=0,input_shape=(None,),name='att_decoder_input_masking')(inp2)\n",
    "    y = inp2\n",
    "    y = tf.keras.layers.Embedding(NUM_CLASSES,dim,name='att_decoder_token_emb')(y) #include sos token\n",
    "    y = PosEmbedding(dim,max_len=target_len,name='att_decoder_pos_emb')(y)\n",
    "    y = TransformerDecoderBlock(dim,expand=2,num_heads=4,attn_dropout=0.2,name='att_decoder_block1')(y,x,x)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(NUM_CLASSES,name='att_decoder_classifier')(y)\n",
    "\n",
    "    decoder = tf.keras.Model([inp2,inp3],y,name='att_decoder')\n",
    "\n",
    "    ################### MODEL #####################\n",
    "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
    "    inp2 = tf.keras.Input((None,),dtype='int32')\n",
    "\n",
    "    x = inp1\n",
    "    enc_out = encoder(x)\n",
    "    y = inp2\n",
    "    dec_out = decoder([y, enc_out])\n",
    "    ctc_out = ctc_decoder(enc_out)\n",
    "    model = tf.keras.Model([inp1,inp2], [dec_out,ctc_out])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b4d4f0-5437-4901-a4a7-eb478f64475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/aslfr-fp16-192d-17l-ctcattjoint-seed42-foldall-last.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2290d5f5-dbfa-455e-8c04-8234a28a0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCGreedyDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model, pad_token_idx=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = model.get_layer('encoder')\n",
    "        self.ctc_decoder = model.get_layer('ctc_decoder')\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        \n",
    "    def decode_phrase(self, pred):\n",
    "        x = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
    "        diff = tf.not_equal(x[:-1], x[1:])\n",
    "        adjacent_indices = tf.where(diff)[:, 0]\n",
    "        x = tf.gather(x, adjacent_indices)\n",
    "        mask = x != self.pad_token_idx\n",
    "        x = tf.boolean_mask(x, mask, axis=0)\n",
    "        return x\n",
    "    \n",
    "    def call(self, batch_x):\n",
    "        encoder_out = self.encoder(batch_x)\n",
    "        ctc_probs = self.ctc_decoder(encoder_out)\n",
    "        return tf.identity([self.decode_phrase(ctc_probs[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb407195-f8d7-4a32-af31-935fac84acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATTGreedyDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model, max_output_length=64, input_strides=2, sos_token_idx=60, eos_token_idx=61, pad_token_idx=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.encoder = self.model.get_layer('encoder')\n",
    "        self.decoder = self.model.get_layer('att_decoder')\n",
    "        self.max_output_length = max_output_length\n",
    "        self.sos_token_idx = sos_token_idx\n",
    "        self.eos_token_idx = eos_token_idx\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        self.input_strides = input_strides\n",
    "\n",
    "    def att_inference_module(self, query, query_position, key_state, value_state, encoder_key_state, encoder_value_state):\n",
    "        x = self.decoder.get_layer('att_decoder_inp1')(query)\n",
    "        x = self.decoder.get_layer('att_decoder_token_emb')(x)\n",
    "        x = self.decoder.get_layer('att_decoder_pos_emb')(x, positions=query_position)\n",
    "\n",
    "        q = x\n",
    "        x = self.decoder.get_layer('att_decoder_block1_bn1')(x)\n",
    "        x, k, v = self.decoder.get_layer('att_decoder_block1_self_attn')(x, x, x, key_state=key_state, value_state=value_state, return_states=True)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_add1')([q,x])\n",
    "        attn_out1 = x\n",
    "\n",
    "        x = self.decoder.get_layer('att_decoder_block1_bn2')(x)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_cross_attn')(x, None, None, key_state=encoder_key_state, value_state=encoder_value_state)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_add2')([attn_out1,x])\n",
    "        attn_out2 = x\n",
    "\n",
    "        x = self.decoder.get_layer('att_decoder_block1_bn3')(x)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_fc1')(x)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_fc2')(x)\n",
    "        x = self.decoder.get_layer('att_decoder_block1_add3')([attn_out2,x])\n",
    "        out = self.decoder.get_layer('att_decoder_classifier')(x)\n",
    "        return out, k, v\n",
    "\n",
    "    def compute_input_length(self, batch_x):\n",
    "        input_length = tf.cast(tf.shape(batch_x)[1], tf.float32)\n",
    "        input_length = tf.math.ceil(input_length/self.input_strides)\n",
    "        return tf.cast(input_length, tf.int32)\n",
    "    \n",
    "    def call(self, batch_x):\n",
    "        encoder_out = self.encoder(batch_x)\n",
    "        input_length = self.compute_input_length(batch_x)\n",
    "        encoder_key_state = self.decoder.get_layer('att_decoder_block1_cross_attn').k(encoder_out)\n",
    "        encoder_value_state = self.decoder.get_layer('att_decoder_block1_cross_attn').v(encoder_out)\n",
    "\n",
    "        time = tf.constant(0, dtype=tf.int32)\n",
    "        predictions = tf.ones((tf.shape(batch_x)[0],1), dtype=tf.int32) * self.sos_token_idx\n",
    "        pad = tf.ones((tf.shape(batch_x)[0],), dtype=tf.int32) * self.pad_token_idx\n",
    "        init = True\n",
    "        key_state = tf.zeros((0,0,192))\n",
    "        value_state = tf.zeros((0,0,192))\n",
    "\n",
    "        def condition(_time, predictions, key_state, value_state, init):\n",
    "            return tf.logical_and(tf.logical_and(_time < self.max_output_length, tf.logical_not(tf.reduce_all(tf.reduce_any(predictions==self.eos_token_idx, axis=1)))), tf.reduce_any(_time < input_length))\n",
    "\n",
    "        def body(_time, predictions, key_state, value_state, init):\n",
    "            if init:\n",
    "                out, key_state, value_state = self.att_inference_module(predictions[:,-1:], _time, None, None, encoder_key_state, encoder_value_state)\n",
    "                init = False\n",
    "            else:\n",
    "                out, key_state, value_state = self.att_inference_module(predictions[:,-1:],  _time, key_state, value_state, encoder_key_state, encoder_value_state)\n",
    "            pred_curr = tf.where(tf.logical_or(tf.reduce_any(predictions==self.eos_token_idx, axis=1), _time >= input_length), pad, tf.argmax(out[:,-1], axis=-1, output_type=tf.int32))\n",
    "            predictions = tf.concat([predictions, pred_curr[...,None]], axis=1)\n",
    "            return _time+1, predictions, key_state, value_state, init\n",
    "\n",
    "        _, predictions, _, _, _ = tf.while_loop(condition, body, \n",
    "                                                shape_invariants=[tf.TensorShape([]),\n",
    "                                                                  tf.TensorShape([None,None]),\n",
    "                                                                  tf.TensorShape([None,None,192]),\n",
    "                                                                  tf.TensorShape([None,None,192]),\n",
    "                                                                  tf.TensorShape([])], \n",
    "                                                loop_vars=[time, predictions, key_state, value_state, init])\n",
    "        print(\"PREDICTIONS[ATT_Decoder]:\\n\",predictions,\"\\n---------------------\\n\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91485675-f68f-4da6-b64e-912b2798e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctc_initial_states(log_probs, blank_idx=0):\n",
    "\n",
    "    blank_probs = log_probs[...,blank_idx]\n",
    "    states_n = tf.ones_like(blank_probs, dtype=tf.float32) * tf.float32.min\n",
    "    states_b = tf.math.cumsum(blank_probs)\n",
    "    \n",
    "    return states_n, states_b\n",
    "    \n",
    "def compute_ctc_prefix_scores(beams, log_probs, states_n, states_b, eos_idx=61, blank_idx=0):\n",
    "    # beams: (N=hypothesis_length)\n",
    "    # probs: (L=(padded/strided)input_length,M=num_models,V=vocab_size)\n",
    "    # states_n: (L,M)\n",
    "    # states_b: (L,M)\n",
    "    \n",
    "    N = tf.shape(beams)[0]\n",
    "    L = tf.shape(states_n)[0]\n",
    "    V = tf.shape(log_probs)[-1]\n",
    "    M = tf.shape(states_n)[1]\n",
    "    new_states_n = tf.ones((L,M,V), dtype=tf.float32) * tf.float32.min\n",
    "    new_states_b = tf.ones((L,M,V), dtype=tf.float32) * tf.float32.min\n",
    "    new_states_n = tf.cond(N==1, lambda:log_probs, lambda:new_states_n)\n",
    "    \n",
    "    r_sum = tf.math.reduce_logsumexp([states_n, states_b], axis=0) #(B,N)\n",
    "    last = beams[-1] #(1,)\n",
    "\n",
    "    repeated_idx = last \n",
    "     \n",
    "    log_phi_ = tf.repeat(r_sum[None,...], repeats=V, axis=0) #(V,L,M)\n",
    "    log_phi = tf.tensor_scatter_nd_update(log_phi_, [[repeated_idx]], [states_b])\n",
    "    log_phi = tf.transpose(log_phi, (1,2,0)) #(L,M,V)\n",
    "    \n",
    "    log_phi = tf.cond(N==1, lambda:tf.transpose(log_phi_, (1,2,0)), lambda:log_phi)\n",
    "    \n",
    "    def step_function(prev, inputs):\n",
    "        prev_r_n, prev_r_b = prev\n",
    "        current_log_phi, current_prob = inputs\n",
    "        updated_r_n = tf.math.reduce_logsumexp([prev_r_n, current_log_phi], axis=0) + current_prob\n",
    "        updated_r_b = tf.math.reduce_logsumexp([prev_r_b, prev_r_n], axis=0) + current_prob[...,blank_idx][...,None]\n",
    "        return updated_r_n, updated_r_b\n",
    "    \n",
    "    start = 1\n",
    "    log_psi = new_states_n[start-1]\n",
    "\n",
    "    sequence_log_phi = log_phi[start-1:L-1]\n",
    "    sequence_probs = log_probs[start-1+N:L-1+N]\n",
    "    sequences = (sequence_log_phi, sequence_probs) #((L-start,M,V), (L-start,M,V))\n",
    "\n",
    "    initial_state = (new_states_n[start-1], new_states_b[start-1]) #((M,V),(M,V),(M,V))\n",
    "    \n",
    "    log_psi = tf.math.reduce_logsumexp([tf.math.reduce_logsumexp(sequence_log_phi + sequence_probs, axis=0), log_psi], axis=0)\n",
    "\n",
    "    new_states_n, new_states_b = tf.scan(step_function, sequences, initial_state)\n",
    "\n",
    "    log_psi_eos = r_sum[-1]\n",
    "    model_idx = tf.range(M)\n",
    "    eos_idxs = tf.stack([model_idx, tf.fill((M,), eos_idx)], axis=-1)\n",
    "    blank_idxs = tf.stack([model_idx, tf.fill((M,), blank_idx)], axis=-1)\n",
    "    log_psi = tf.tensor_scatter_nd_update(log_psi, eos_idxs, log_psi_eos)\n",
    "    log_psi = tf.tensor_scatter_nd_update(log_psi, blank_idxs, tf.fill((M,), tf.float32.min))\n",
    "    \n",
    "    print(\"\\n Log_psi:\",log_psi,\"\\nnew_States_n:\",new_states_n,\"\\nnew_states_b:\",new_states_b)\n",
    "    return log_psi, new_states_n, new_states_b #(M,V), (L,M,V), (L,M,V)\n",
    "\n",
    "\n",
    "class EnsembleCTCAttentionJointGreedyDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_list, ctc_weight=0.2, input_strides=2, max_output_length=64, blank_idx=0, pad_frame_idx=-100, sos_token_idx=60, eos_token_idx=61, pad_token_idx=0, from_logits=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_list = [m.get_layer('encoder') for m in model_list]\n",
    "        self.decoder_list = [m.get_layer('att_decoder') for m in model_list]\n",
    "        self.ctc_decoder_list = [m.get_layer('ctc_decoder') for m in model_list]\n",
    "        self.ctc_weight = ctc_weight\n",
    "        self.input_strides = input_strides\n",
    "        self.max_output_length = max_output_length\n",
    "        self.blank_idx = blank_idx\n",
    "        self.pad_frame_idx = pad_frame_idx\n",
    "        self.sos_token_idx = sos_token_idx\n",
    "        self.eos_token_idx = eos_token_idx\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        self.from_logits = from_logits\n",
    "        \n",
    "    def compute_input_length(self, batch_x):\n",
    "        input_length = tf.cast(tf.shape(batch_x)[1], tf.float32)#tf.reduce_sum(tf.cast(mask, tf.float32), axis=-1)\n",
    "        input_length = tf.math.ceil(input_length/self.input_strides)\n",
    "        return tf.cast(input_length, tf.int32)\n",
    "\n",
    "    def att_inference_module(self, query, query_position, key_state_list, value_state_list, encoder_key_state_list, encoder_value_state_list):\n",
    "        outputs = []\n",
    "        key_states = []\n",
    "        value_states = []\n",
    "        for i in range(len(self.decoder_list)):\n",
    "            decoder = self.decoder_list[i]\n",
    "            key_state = key_state_list[i] if key_state_list is not None else None\n",
    "            value_state = value_state_list[i] if value_state_list is not None else None\n",
    "            encoder_key_state = encoder_key_state_list[i] if encoder_key_state_list is not None else None\n",
    "            encoder_value_state = encoder_value_state_list[i] if encoder_value_state_list is not None else None\n",
    "            x = decoder.get_layer('att_decoder_inp1')(query)\n",
    "            x = decoder.get_layer('att_decoder_token_emb')(x)\n",
    "            x = decoder.get_layer('att_decoder_pos_emb')(x, positions=query_position)\n",
    "\n",
    "            q = x\n",
    "            x = decoder.get_layer('att_decoder_block1_bn1')(x)\n",
    "            x, k, v = decoder.get_layer('att_decoder_block1_self_attn')(x, x, x, key_state=key_state, value_state=value_state, return_states=True)\n",
    "            x = decoder.get_layer('att_decoder_block1_add1')([q,x])\n",
    "            attn_out1 = x\n",
    "\n",
    "            x = decoder.get_layer('att_decoder_block1_bn2')(x)\n",
    "            x = decoder.get_layer('att_decoder_block1_cross_attn')(x, None, None, key_state=encoder_key_state, value_state=encoder_value_state)\n",
    "            x = decoder.get_layer('att_decoder_block1_add2')([attn_out1,x])\n",
    "            attn_out2 = x\n",
    "\n",
    "            x = decoder.get_layer('att_decoder_block1_bn3')(x)\n",
    "            x = decoder.get_layer('att_decoder_block1_fc1')(x)\n",
    "            x = decoder.get_layer('att_decoder_block1_fc2')(x)\n",
    "            x = decoder.get_layer('att_decoder_block1_add3')([attn_out2,x])\n",
    "            out = decoder.get_layer('att_decoder_classifier')(x)\n",
    "            outputs.append(out)\n",
    "            key_states.append(k)\n",
    "            value_states.append(v)\n",
    "        return tf.identity(outputs), tf.identity(key_states), tf.identity(value_states)\n",
    "\n",
    "    def get_initial_states(self, batch_x):\n",
    "        encoder_outputs = [enc(batch_x) for enc in self.encoder_list]\n",
    "        encoder_key_states = [dec.get_layer('att_decoder_block1_cross_attn').k(x) for dec, x in zip(self.decoder_list, encoder_outputs)]\n",
    "        encoder_value_states = [dec.get_layer('att_decoder_block1_cross_attn').v(x) for dec, x in zip(self.decoder_list, encoder_outputs)]\n",
    "        key_states = [tf.zeros((0,0,192)) for _ in self.encoder_list]\n",
    "        value_states = [tf.zeros((0,0,192)) for _ in self.encoder_list]\n",
    "        ctc_probs = [dec(x)[0] for dec,x in zip(self.ctc_decoder_list, encoder_outputs)]\n",
    "        \n",
    "        encoder_key_states = tf.stack(encoder_key_states)\n",
    "        encoder_value_states = tf.stack(encoder_value_states)\n",
    "        key_states = tf.stack(key_states)\n",
    "        value_states = tf.stack(value_states)\n",
    "        encoder_outputs = tf.stack(encoder_outputs)\n",
    "        \n",
    "        if self.from_logits:\n",
    "            ctc_probs = [tf.nn.softmax(x, axis=-1) for x in ctc_probs]\n",
    "        ctc_probs = tf.stack([tf.math.log(x) for x in ctc_probs], axis=1)\n",
    "        ctc_states_n, ctc_states_b = get_ctc_initial_states(ctc_probs, self.blank_idx)\n",
    "        return encoder_key_states, encoder_value_states, key_states, value_states, ctc_probs, ctc_states_n, ctc_states_b\n",
    "\n",
    "    def call(self, batch_x):\n",
    "\n",
    "        encoder_key_state, encoder_value_state, key_state, value_state, ctc_log_probs, ctc_states_n, ctc_states_b = self.get_initial_states(batch_x)\n",
    "        input_length = self.compute_input_length(batch_x)\n",
    "\n",
    "        time = tf.constant(0, dtype=tf.int32)\n",
    "        predictions = tf.ones((tf.shape(batch_x)[0],1), dtype=tf.int32) * self.sos_token_idx#tf.TensorArray(dtype=tf.int32,size=self.max_output_length)\n",
    "        pad = tf.ones((tf.shape(batch_x)[0],), dtype=tf.int32) * self.pad_token_idx\n",
    "        init = True\n",
    "\n",
    "        def condition(_time, predictions, ctc_states_n, ctc_states_b, key_state, value_state, init):\n",
    "            return tf.logical_and(_time < tf.minimum(self.max_output_length, input_length), tf.logical_not(tf.reduce_all(tf.reduce_any(predictions==self.eos_token_idx, axis=1))))\n",
    "\n",
    "        def body(_time, predictions, ctc_states_n, ctc_states_b, key_state, value_state, init):\n",
    "            if init:\n",
    "                out, key_state, value_state = self.att_inference_module(predictions[:,-1:], _time, None, None, encoder_key_state, encoder_value_state)\n",
    "                init = False\n",
    "            else:\n",
    "                out, key_state, value_state = self.att_inference_module(predictions[:,-1:],  _time, key_state, value_state, encoder_key_state, encoder_value_state)\n",
    "\n",
    "            log_ctc, new_ctc_states_n, new_ctc_states_b = compute_ctc_prefix_scores(predictions[0], \n",
    "                                                                                   ctc_log_probs, \n",
    "                                                                                   ctc_states_n, \n",
    "                                                                                   ctc_states_b, \n",
    "                                                                                   self.eos_token_idx, \n",
    "                                                                                   self.blank_idx)\n",
    "            log_ctc = tf.reduce_mean(log_ctc, axis=0) #log-prob ensemble\n",
    "            out = out[:,0,0] #(M,V)\n",
    "            if self.from_logits:\n",
    "                out = tf.nn.softmax(out, axis=-1)\n",
    "            out = tf.math.log(out)\n",
    "            log_att = tf.reduce_mean(out, axis=0) #log-prob ensemble\n",
    "            \n",
    "            probs_final = (1-self.ctc_weight) * log_att + self.ctc_weight * log_ctc #tf.expand_dims(log_psi, axis=0)\n",
    "            next_token = tf.argmax(probs_final, axis=-1, output_type=tf.int32)#[0]\n",
    "            \n",
    "            ctc_states_n = new_ctc_states_n[...,next_token]\n",
    "            ctc_states_b = new_ctc_states_b[...,next_token]\n",
    "            \n",
    "            predictions = tf.concat([predictions, [next_token[...,None]]], axis=1)\n",
    "            return _time+1, predictions, ctc_states_n, ctc_states_b, key_state, value_state, init\n",
    "\n",
    "        _, predictions, _, _, _, _, _ = tf.while_loop(condition, body, \n",
    "                                                shape_invariants=[tf.TensorShape([]),\n",
    "                                                                  tf.TensorShape([1,None]),\n",
    "                                                                  tf.TensorShape([None,len(self.encoder_list)]),\n",
    "                                                                  tf.TensorShape([None,len(self.encoder_list)]),\n",
    "                                                                  tf.TensorShape([len(self.encoder_list),None,None,None]),\n",
    "                                                                  tf.TensorShape([len(self.encoder_list),None,None,None]),\n",
    "                                                                  tf.TensorShape([])], \n",
    "                                                loop_vars=[time, predictions, ctc_states_n, ctc_states_b, key_state, value_state, init])\n",
    "        print(\"PREDICTIONS[CTC_Decoder]:\\n\",predictions,\"\\n---------------------\\n\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a3d9a-a5b2-4bb7-b706-134c9e90d52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2525ccee-e2ae-4c85-a8ad-1e46eaa49d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 768, 1629)]          0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, 384, 192)             5565377   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " att_decoder (Functional)    (None, None, 62)             480830    ['input_3[0][0]',             \n",
      "                                                                     'encoder[0][0]']             \n",
      "                                                                                                  \n",
      " ctc_decoder (Functional)    (None, 384, 62)              320318    ['encoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6366525 (24.29 MB)\n",
      "Trainable params: 6336957 (24.17 MB)\n",
      "Non-trainable params: 29568 (115.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "beacf211-46ca-4420-a7e5-f366fddcd282",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tflitemodel_base \u001b[38;5;241m=\u001b[39m TFLiteModel(EnsembleCTCAttentionJointGreedyDecoder(\u001b[43mmodel_list\u001b[49m[:\u001b[38;5;241m3\u001b[39m], ctc_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_list' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296acd5-dc4f-4dbd-93ef-e756ead208f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc271299-c5ed-41fb-9f15-1241dac13080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58d2fb3e-9dab-405e-aa95-295afc70441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb4a3f70-ba97-42ab-a1b7-d437badd2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTEGRATION WITH CAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2d517e37-f7a0-4723-8a4e-b485a98438b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c7050dfb-0c58-4993-9df1-e2a26ce9904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "07b76115-46d2-4ec4-9000-d57fe6e07be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "frame_counter = 0\n",
    "\n",
    "all_landmarks_list = []\n",
    "\n",
    "# Initialize drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6351db75-c0dc-4bd6-8d76-7ca14632f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "while capture.isOpened() and frame_counter < 100:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Use holistic model to detect landmarks\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Convert back to BGR for rendering\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,255), thickness=1, circle_radius=1),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,255), thickness=1, circle_radius=1)\n",
    "    )\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, \n",
    "        results.right_hand_landmarks, \n",
    "        mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, \n",
    "        results.left_hand_landmarks, \n",
    "        mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    # Display the resulting image with landmarks\n",
    "    cv2.imshow('Holistic Model Landmarks', image)\n",
    "\n",
    "    all_landmarks = []\n",
    "\n",
    "    # Extract pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "        all_landmarks.extend(pose_landmarks)\n",
    "\n",
    "    # Extract face landmarks\n",
    "    if results.face_landmarks:\n",
    "        face_landmarks = [[lm.x, lm.y, lm.z] for lm in results.face_landmarks.landmark]\n",
    "        all_landmarks.extend(face_landmarks)\n",
    "\n",
    "    # Extract left hand landmarks\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]\n",
    "        all_landmarks.extend(left_hand_landmarks)\n",
    "\n",
    "    # Extract right hand landmarks\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]\n",
    "        all_landmarks.extend(right_hand_landmarks)\n",
    "\n",
    "    # Append the landmarks of this frame to the list\n",
    "    all_landmarks_list.append(all_landmarks)\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "635b105d-8325-41cd-b397-3b21ec182188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all landmarks tensor before reshaping: (100, 543, 3)\n",
      "Shape of all landmarks tensor after reshaping: (100, 1629)\n"
     ]
    }
   ],
   "source": [
    "# Define the expected order of landmarks\n",
    "expected_landmark_order = []\n",
    "\n",
    "# Add face landmarks (assuming 468 landmarks)\n",
    "for i in range(468):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add right hand landmarks (assuming 21 landmarks)\n",
    "for i in range(468, 468 + 21):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add left hand landmarks (assuming 21 landmarks)\n",
    "for i in range(468 + 21, 468 + 21 + 21):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add pose landmarks (assuming 33 landmarks)\n",
    "for i in range(468 + 21 + 21, 468 + 21 + 21 + 33):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Find the maximum number of landmarks\n",
    "max_landmarks = max(len(landmarks) for landmarks in all_landmarks_list)\n",
    "# Ensure that the shape is (100, 543, 3) by padding with NaN values\n",
    "padded_landmarks = []\n",
    "for landmarks in all_landmarks_list:\n",
    "    padded_landmarks.append(landmarks + [[np.nan, np.nan, np.nan]] * (543 - len(landmarks)))\n",
    "\n",
    "# Convert the list of landmarks to a TensorFlow tensor\n",
    "all_landmarks_tensor = tf.convert_to_tensor(padded_landmarks, dtype=tf.float32)\n",
    "\n",
    "print(\"Shape of all landmarks tensor before reshaping:\", all_landmarks_tensor.shape)\n",
    "\n",
    "# Reshape the tensor to have shape (100, 1629)\n",
    "all_landmarks_tensor_reshaped = tf.reshape(all_landmarks_tensor, (100, -1))\n",
    "\n",
    "print(\"Shape of all landmarks tensor after reshaping:\", all_landmarks_tensor_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c37471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "27778744-8190-48f2-ab88-75779d4583d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'166 south road'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = prediction_fn(inputs=all_landmarks_tensor_reshaped)\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "prediction_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d2c0f-fe0b-4c97-b04b-bcb1ce85551e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "57607e89-5daf-4272-baf5-8d47d90a5e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rows = []\n",
    "\n",
    "# for i in range(all_landmarks_tensor_reshaped.shape[0]):\n",
    "#     # Extract the i-th row\n",
    "#     row = all_landmarks_tensor_reshaped[i:i+1, :]\n",
    "#     # Append the row to the list\n",
    "#     print(row.shape)\n",
    "#     predictions = model.predict(row)\n",
    "#     rows.append(row)\n",
    "# print(rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da4ab752-4bad-4137-921a-2e7837b6d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_tensor = tf.expand_dims(all_landmarks_tensor_reshaped, axis=0)\n",
    "# output_tensor\n",
    "\n",
    "# predictions = model.predict(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a155be6-b69a-4ba8-b567-0b96612639ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 768, 1629) dtype=float32 (created by layer 'input_8')>, <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_9')>]\n"
     ]
    }
   ],
   "source": [
    "print(model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f0ea7-aeb6-4644-bfed-e4f080a41644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620bd6e-f973-4770-bb1e-98c0affdac87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528e342-f355-43e8-9eee-3f9a9a777acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5f21cb2-b3a5-47a6-a133-a62cc50129de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEL_COLS=['x_face_0', 'x_face_1', 'x_face_2', 'x_face_3', 'x_face_4', 'x_face_5', 'x_face_6', 'x_face_7', 'x_face_8', 'x_face_9', 'x_face_10', 'x_face_11', 'x_face_12', 'x_face_13', 'x_face_14', 'x_face_15', 'x_face_16', 'x_face_17', 'x_face_18', 'x_face_19', 'x_face_20', 'x_face_21', 'x_face_22', 'x_face_23', 'x_face_24', 'x_face_25', 'x_face_26', 'x_face_27', 'x_face_28', 'x_face_29', 'x_face_30', 'x_face_31', 'x_face_32', 'x_face_33', 'x_face_34', 'x_face_35', 'x_face_36', 'x_face_37', 'x_face_38', 'x_face_39', 'x_face_40', 'x_face_41', 'x_face_42', 'x_face_43', 'x_face_44', 'x_face_45', 'x_face_46', 'x_face_47', 'x_face_48', 'x_face_49', 'x_face_50', 'x_face_51', 'x_face_52', 'x_face_53', 'x_face_54', 'x_face_55', 'x_face_56', 'x_face_57', 'x_face_58', 'x_face_59', 'x_face_60', 'x_face_61', 'x_face_62', 'x_face_63', 'x_face_64', 'x_face_65', 'x_face_66', 'x_face_67', 'x_face_68', 'x_face_69', 'x_face_70', 'x_face_71', 'x_face_72', 'x_face_73', 'x_face_74', 'x_face_75', 'x_face_76', 'x_face_77', 'x_face_78', 'x_face_79', 'x_face_80', 'x_face_81', 'x_face_82', 'x_face_83', 'x_face_84', 'x_face_85', 'x_face_86', 'x_face_87', 'x_face_88', 'x_face_89', 'x_face_90', 'x_face_91', 'x_face_92', 'x_face_93', 'x_face_94', 'x_face_95', 'x_face_96', 'x_face_97', 'x_face_98', 'x_face_99', 'x_face_100', 'x_face_101', 'x_face_102', 'x_face_103', 'x_face_104', 'x_face_105', 'x_face_106', 'x_face_107', 'x_face_108', 'x_face_109', 'x_face_110', 'x_face_111', 'x_face_112', 'x_face_113', 'x_face_114', 'x_face_115', 'x_face_116', 'x_face_117', 'x_face_118', 'x_face_119', 'x_face_120', 'x_face_121', 'x_face_122', 'x_face_123', 'x_face_124', 'x_face_125', 'x_face_126', 'x_face_127', 'x_face_128', 'x_face_129', 'x_face_130', 'x_face_131', 'x_face_132', 'x_face_133', 'x_face_134', 'x_face_135', 'x_face_136', 'x_face_137', 'x_face_138', 'x_face_139', 'x_face_140', 'x_face_141', 'x_face_142', 'x_face_143', 'x_face_144', 'x_face_145', 'x_face_146', 'x_face_147', 'x_face_148', 'x_face_149', 'x_face_150', 'x_face_151', 'x_face_152', 'x_face_153', 'x_face_154', 'x_face_155', 'x_face_156', 'x_face_157', 'x_face_158', 'x_face_159', 'x_face_160', 'x_face_161', 'x_face_162', 'x_face_163', 'x_face_164', 'x_face_165', 'x_face_166', 'x_face_167', 'x_face_168', 'x_face_169', 'x_face_170', 'x_face_171', 'x_face_172', 'x_face_173', 'x_face_174', 'x_face_175', 'x_face_176', 'x_face_177', 'x_face_178', 'x_face_179', 'x_face_180', 'x_face_181', 'x_face_182', 'x_face_183', 'x_face_184', 'x_face_185', 'x_face_186', 'x_face_187', 'x_face_188', 'x_face_189', 'x_face_190', 'x_face_191', 'x_face_192', 'x_face_193', 'x_face_194', 'x_face_195', 'x_face_196', 'x_face_197', 'x_face_198', 'x_face_199', 'x_face_200', 'x_face_201', 'x_face_202', 'x_face_203', 'x_face_204', 'x_face_205', 'x_face_206', 'x_face_207', 'x_face_208', 'x_face_209', 'x_face_210', 'x_face_211', 'x_face_212', 'x_face_213', 'x_face_214', 'x_face_215', 'x_face_216', 'x_face_217', 'x_face_218', 'x_face_219', 'x_face_220', 'x_face_221', 'x_face_222', 'x_face_223', 'x_face_224', 'x_face_225', 'x_face_226', 'x_face_227', 'x_face_228', 'x_face_229', 'x_face_230', 'x_face_231', 'x_face_232', 'x_face_233', 'x_face_234', 'x_face_235', 'x_face_236', 'x_face_237', 'x_face_238', 'x_face_239', 'x_face_240', 'x_face_241', 'x_face_242', 'x_face_243', 'x_face_244', 'x_face_245', 'x_face_246', 'x_face_247', 'x_face_248', 'x_face_249', 'x_face_250', 'x_face_251', 'x_face_252', 'x_face_253', 'x_face_254', 'x_face_255', 'x_face_256', 'x_face_257', 'x_face_258', 'x_face_259', 'x_face_260', 'x_face_261', 'x_face_262', 'x_face_263', 'x_face_264', 'x_face_265', 'x_face_266', 'x_face_267', 'x_face_268', 'x_face_269', 'x_face_270', 'x_face_271', 'x_face_272', 'x_face_273', 'x_face_274', 'x_face_275', 'x_face_276', 'x_face_277', 'x_face_278', 'x_face_279', 'x_face_280', 'x_face_281', 'x_face_282', 'x_face_283', 'x_face_284', 'x_face_285', 'x_face_286', 'x_face_287', 'x_face_288', 'x_face_289', 'x_face_290', 'x_face_291', 'x_face_292', 'x_face_293', 'x_face_294', 'x_face_295', 'x_face_296', 'x_face_297', 'x_face_298', 'x_face_299', 'x_face_300', 'x_face_301', 'x_face_302', 'x_face_303', 'x_face_304', 'x_face_305', 'x_face_306', 'x_face_307', 'x_face_308', 'x_face_309', 'x_face_310', 'x_face_311', 'x_face_312', 'x_face_313', 'x_face_314', 'x_face_315', 'x_face_316', 'x_face_317', 'x_face_318', 'x_face_319', 'x_face_320', 'x_face_321', 'x_face_322', 'x_face_323', 'x_face_324', 'x_face_325', 'x_face_326', 'x_face_327', 'x_face_328', 'x_face_329', 'x_face_330', 'x_face_331', 'x_face_332', 'x_face_333', 'x_face_334', 'x_face_335', 'x_face_336', 'x_face_337', 'x_face_338', 'x_face_339', 'x_face_340', 'x_face_341', 'x_face_342', 'x_face_343', 'x_face_344', 'x_face_345', 'x_face_346', 'x_face_347', 'x_face_348', 'x_face_349', 'x_face_350', 'x_face_351', 'x_face_352', 'x_face_353', 'x_face_354', 'x_face_355', 'x_face_356', 'x_face_357', 'x_face_358', 'x_face_359', 'x_face_360', 'x_face_361', 'x_face_362', 'x_face_363', 'x_face_364', 'x_face_365', 'x_face_366', 'x_face_367', 'x_face_368', 'x_face_369', 'x_face_370', 'x_face_371', 'x_face_372', 'x_face_373', 'x_face_374', 'x_face_375', 'x_face_376', 'x_face_377', 'x_face_378', 'x_face_379', 'x_face_380', 'x_face_381', 'x_face_382', 'x_face_383', 'x_face_384', 'x_face_385', 'x_face_386', 'x_face_387', 'x_face_388', 'x_face_389', 'x_face_390', 'x_face_391', 'x_face_392', 'x_face_393', 'x_face_394', 'x_face_395', 'x_face_396', 'x_face_397', 'x_face_398', 'x_face_399', 'x_face_400', 'x_face_401', 'x_face_402', 'x_face_403', 'x_face_404', 'x_face_405', 'x_face_406', 'x_face_407', 'x_face_408', 'x_face_409', 'x_face_410', 'x_face_411', 'x_face_412', 'x_face_413', 'x_face_414', 'x_face_415', 'x_face_416', 'x_face_417', 'x_face_418', 'x_face_419', 'x_face_420', 'x_face_421', 'x_face_422', 'x_face_423', 'x_face_424', 'x_face_425', 'x_face_426', 'x_face_427', 'x_face_428', 'x_face_429', 'x_face_430', 'x_face_431', 'x_face_432', 'x_face_433', 'x_face_434', 'x_face_435', 'x_face_436', 'x_face_437', 'x_face_438', 'x_face_439', 'x_face_440', 'x_face_441', 'x_face_442', 'x_face_443', 'x_face_444', 'x_face_445', 'x_face_446', 'x_face_447', 'x_face_448', 'x_face_449', 'x_face_450', 'x_face_451', 'x_face_452', 'x_face_453', 'x_face_454', 'x_face_455', 'x_face_456', 'x_face_457', 'x_face_458', 'x_face_459', 'x_face_460', 'x_face_461', 'x_face_462', 'x_face_463', 'x_face_464', 'x_face_465', 'x_face_466', 'x_face_467', 'x_left_hand_0', 'x_left_hand_1', 'x_left_hand_2', 'x_left_hand_3', 'x_left_hand_4', 'x_left_hand_5', 'x_left_hand_6', 'x_left_hand_7', 'x_left_hand_8', 'x_left_hand_9', 'x_left_hand_10', 'x_left_hand_11', 'x_left_hand_12', 'x_left_hand_13', 'x_left_hand_14', 'x_left_hand_15', 'x_left_hand_16', 'x_left_hand_17', 'x_left_hand_18', 'x_left_hand_19', 'x_left_hand_20', 'x_pose_0', 'x_pose_1', 'x_pose_2', 'x_pose_3', 'x_pose_4', 'x_pose_5', 'x_pose_6', 'x_pose_7', 'x_pose_8', 'x_pose_9', 'x_pose_10', 'x_pose_11', 'x_pose_12', 'x_pose_13', 'x_pose_14', 'x_pose_15', 'x_pose_16', 'x_pose_17', 'x_pose_18', 'x_pose_19', 'x_pose_20', 'x_pose_21', 'x_pose_22', 'x_pose_23', 'x_pose_24', 'x_pose_25', 'x_pose_26', 'x_pose_27', 'x_pose_28', 'x_pose_29', 'x_pose_30', 'x_pose_31', 'x_pose_32', 'x_right_hand_0', 'x_right_hand_1', 'x_right_hand_2', 'x_right_hand_3', 'x_right_hand_4', 'x_right_hand_5', 'x_right_hand_6', 'x_right_hand_7', 'x_right_hand_8', 'x_right_hand_9', 'x_right_hand_10', 'x_right_hand_11', 'x_right_hand_12', 'x_right_hand_13', 'x_right_hand_14', 'x_right_hand_15', 'x_right_hand_16', 'x_right_hand_17', 'x_right_hand_18', 'x_right_hand_19', 'x_right_hand_20', 'y_face_0', 'y_face_1', 'y_face_2', 'y_face_3', 'y_face_4', 'y_face_5', 'y_face_6', 'y_face_7', 'y_face_8', 'y_face_9', 'y_face_10', 'y_face_11', 'y_face_12', 'y_face_13', 'y_face_14', 'y_face_15', 'y_face_16', 'y_face_17', 'y_face_18', 'y_face_19', 'y_face_20', 'y_face_21', 'y_face_22', 'y_face_23', 'y_face_24', 'y_face_25', 'y_face_26', 'y_face_27', 'y_face_28', 'y_face_29', 'y_face_30', 'y_face_31', 'y_face_32', 'y_face_33', 'y_face_34', 'y_face_35', 'y_face_36', 'y_face_37', 'y_face_38', 'y_face_39', 'y_face_40', 'y_face_41', 'y_face_42', 'y_face_43', 'y_face_44', 'y_face_45', 'y_face_46', 'y_face_47', 'y_face_48', 'y_face_49', 'y_face_50', 'y_face_51', 'y_face_52', 'y_face_53', 'y_face_54', 'y_face_55', 'y_face_56', 'y_face_57', 'y_face_58', 'y_face_59', 'y_face_60', 'y_face_61', 'y_face_62', 'y_face_63', 'y_face_64', 'y_face_65', 'y_face_66', 'y_face_67', 'y_face_68', 'y_face_69', 'y_face_70', 'y_face_71', 'y_face_72', 'y_face_73', 'y_face_74', 'y_face_75', 'y_face_76', 'y_face_77', 'y_face_78', 'y_face_79', 'y_face_80', 'y_face_81', 'y_face_82', 'y_face_83', 'y_face_84', 'y_face_85', 'y_face_86', 'y_face_87', 'y_face_88', 'y_face_89', 'y_face_90', 'y_face_91', 'y_face_92', 'y_face_93', 'y_face_94', 'y_face_95', 'y_face_96', 'y_face_97', 'y_face_98', 'y_face_99', 'y_face_100', 'y_face_101', 'y_face_102', 'y_face_103', 'y_face_104', 'y_face_105', 'y_face_106', 'y_face_107', 'y_face_108', 'y_face_109', 'y_face_110', 'y_face_111', 'y_face_112', 'y_face_113', 'y_face_114', 'y_face_115', 'y_face_116', 'y_face_117', 'y_face_118', 'y_face_119', 'y_face_120', 'y_face_121', 'y_face_122', 'y_face_123', 'y_face_124', 'y_face_125', 'y_face_126', 'y_face_127', 'y_face_128', 'y_face_129', 'y_face_130', 'y_face_131', 'y_face_132', 'y_face_133', 'y_face_134', 'y_face_135', 'y_face_136', 'y_face_137', 'y_face_138', 'y_face_139', 'y_face_140', 'y_face_141', 'y_face_142', 'y_face_143', 'y_face_144', 'y_face_145', 'y_face_146', 'y_face_147', 'y_face_148', 'y_face_149', 'y_face_150', 'y_face_151', 'y_face_152', 'y_face_153', 'y_face_154', 'y_face_155', 'y_face_156', 'y_face_157', 'y_face_158', 'y_face_159', 'y_face_160', 'y_face_161', 'y_face_162', 'y_face_163', 'y_face_164', 'y_face_165', 'y_face_166', 'y_face_167', 'y_face_168', 'y_face_169', 'y_face_170', 'y_face_171', 'y_face_172', 'y_face_173', 'y_face_174', 'y_face_175', 'y_face_176', 'y_face_177', 'y_face_178', 'y_face_179', 'y_face_180', 'y_face_181', 'y_face_182', 'y_face_183', 'y_face_184', 'y_face_185', 'y_face_186', 'y_face_187', 'y_face_188', 'y_face_189', 'y_face_190', 'y_face_191', 'y_face_192', 'y_face_193', 'y_face_194', 'y_face_195', 'y_face_196', 'y_face_197', 'y_face_198', 'y_face_199', 'y_face_200', 'y_face_201', 'y_face_202', 'y_face_203', 'y_face_204', 'y_face_205', 'y_face_206', 'y_face_207', 'y_face_208', 'y_face_209', 'y_face_210', 'y_face_211', 'y_face_212', 'y_face_213', 'y_face_214', 'y_face_215', 'y_face_216', 'y_face_217', 'y_face_218', 'y_face_219', 'y_face_220', 'y_face_221', 'y_face_222', 'y_face_223', 'y_face_224', 'y_face_225', 'y_face_226', 'y_face_227', 'y_face_228', 'y_face_229', 'y_face_230', 'y_face_231', 'y_face_232', 'y_face_233', 'y_face_234', 'y_face_235', 'y_face_236', 'y_face_237', 'y_face_238', 'y_face_239', 'y_face_240', 'y_face_241', 'y_face_242', 'y_face_243', 'y_face_244', 'y_face_245', 'y_face_246', 'y_face_247', 'y_face_248', 'y_face_249', 'y_face_250', 'y_face_251', 'y_face_252', 'y_face_253', 'y_face_254', 'y_face_255', 'y_face_256', 'y_face_257', 'y_face_258', 'y_face_259', 'y_face_260', 'y_face_261', 'y_face_262', 'y_face_263', 'y_face_264', 'y_face_265', 'y_face_266', 'y_face_267', 'y_face_268', 'y_face_269', 'y_face_270', 'y_face_271', 'y_face_272', 'y_face_273', 'y_face_274', 'y_face_275', 'y_face_276', 'y_face_277', 'y_face_278', 'y_face_279', 'y_face_280', 'y_face_281', 'y_face_282', 'y_face_283', 'y_face_284', 'y_face_285', 'y_face_286', 'y_face_287', 'y_face_288', 'y_face_289', 'y_face_290', 'y_face_291', 'y_face_292', 'y_face_293', 'y_face_294', 'y_face_295', 'y_face_296', 'y_face_297', 'y_face_298', 'y_face_299', 'y_face_300', 'y_face_301', 'y_face_302', 'y_face_303', 'y_face_304', 'y_face_305', 'y_face_306', 'y_face_307', 'y_face_308', 'y_face_309', 'y_face_310', 'y_face_311', 'y_face_312', 'y_face_313', 'y_face_314', 'y_face_315', 'y_face_316', 'y_face_317', 'y_face_318', 'y_face_319', 'y_face_320', 'y_face_321', 'y_face_322', 'y_face_323', 'y_face_324', 'y_face_325', 'y_face_326', 'y_face_327', 'y_face_328', 'y_face_329', 'y_face_330', 'y_face_331', 'y_face_332', 'y_face_333', 'y_face_334', 'y_face_335', 'y_face_336', 'y_face_337', 'y_face_338', 'y_face_339', 'y_face_340', 'y_face_341', 'y_face_342', 'y_face_343', 'y_face_344', 'y_face_345', 'y_face_346', 'y_face_347', 'y_face_348', 'y_face_349', 'y_face_350', 'y_face_351', 'y_face_352', 'y_face_353', 'y_face_354', 'y_face_355', 'y_face_356', 'y_face_357', 'y_face_358', 'y_face_359', 'y_face_360', 'y_face_361', 'y_face_362', 'y_face_363', 'y_face_364', 'y_face_365', 'y_face_366', 'y_face_367', 'y_face_368', 'y_face_369', 'y_face_370', 'y_face_371', 'y_face_372', 'y_face_373', 'y_face_374', 'y_face_375', 'y_face_376', 'y_face_377', 'y_face_378', 'y_face_379', 'y_face_380', 'y_face_381', 'y_face_382', 'y_face_383', 'y_face_384', 'y_face_385', 'y_face_386', 'y_face_387', 'y_face_388', 'y_face_389', 'y_face_390', 'y_face_391', 'y_face_392', 'y_face_393', 'y_face_394', 'y_face_395', 'y_face_396', 'y_face_397', 'y_face_398', 'y_face_399', 'y_face_400', 'y_face_401', 'y_face_402', 'y_face_403', 'y_face_404', 'y_face_405', 'y_face_406', 'y_face_407', 'y_face_408', 'y_face_409', 'y_face_410', 'y_face_411', 'y_face_412', 'y_face_413', 'y_face_414', 'y_face_415', 'y_face_416', 'y_face_417', 'y_face_418', 'y_face_419', 'y_face_420', 'y_face_421', 'y_face_422', 'y_face_423', 'y_face_424', 'y_face_425', 'y_face_426', 'y_face_427', 'y_face_428', 'y_face_429', 'y_face_430', 'y_face_431', 'y_face_432', 'y_face_433', 'y_face_434', 'y_face_435', 'y_face_436', 'y_face_437', 'y_face_438', 'y_face_439', 'y_face_440', 'y_face_441', 'y_face_442', 'y_face_443', 'y_face_444', 'y_face_445', 'y_face_446', 'y_face_447', 'y_face_448', 'y_face_449', 'y_face_450', 'y_face_451', 'y_face_452', 'y_face_453', 'y_face_454', 'y_face_455', 'y_face_456', 'y_face_457', 'y_face_458', 'y_face_459', 'y_face_460', 'y_face_461', 'y_face_462', 'y_face_463', 'y_face_464', 'y_face_465', 'y_face_466', 'y_face_467', 'y_left_hand_0', 'y_left_hand_1', 'y_left_hand_2', 'y_left_hand_3', 'y_left_hand_4', 'y_left_hand_5', 'y_left_hand_6', 'y_left_hand_7', 'y_left_hand_8', 'y_left_hand_9', 'y_left_hand_10', 'y_left_hand_11', 'y_left_hand_12', 'y_left_hand_13', 'y_left_hand_14', 'y_left_hand_15', 'y_left_hand_16', 'y_left_hand_17', 'y_left_hand_18', 'y_left_hand_19', 'y_left_hand_20', 'y_pose_0', 'y_pose_1', 'y_pose_2', 'y_pose_3', 'y_pose_4', 'y_pose_5', 'y_pose_6', 'y_pose_7', 'y_pose_8', 'y_pose_9', 'y_pose_10', 'y_pose_11', 'y_pose_12', 'y_pose_13', 'y_pose_14', 'y_pose_15', 'y_pose_16', 'y_pose_17', 'y_pose_18', 'y_pose_19', 'y_pose_20', 'y_pose_21', 'y_pose_22', 'y_pose_23', 'y_pose_24', 'y_pose_25', 'y_pose_26', 'y_pose_27', 'y_pose_28', 'y_pose_29', 'y_pose_30', 'y_pose_31', 'y_pose_32', 'y_right_hand_0', 'y_right_hand_1', 'y_right_hand_2', 'y_right_hand_3', 'y_right_hand_4', 'y_right_hand_5', 'y_right_hand_6', 'y_right_hand_7', 'y_right_hand_8', 'y_right_hand_9', 'y_right_hand_10', 'y_right_hand_11', 'y_right_hand_12', 'y_right_hand_13', 'y_right_hand_14', 'y_right_hand_15', 'y_right_hand_16', 'y_right_hand_17', 'y_right_hand_18', 'y_right_hand_19', 'y_right_hand_20', 'z_face_0', 'z_face_1', 'z_face_2', 'z_face_3', 'z_face_4', 'z_face_5', 'z_face_6', 'z_face_7', 'z_face_8', 'z_face_9', 'z_face_10', 'z_face_11', 'z_face_12', 'z_face_13', 'z_face_14', 'z_face_15', 'z_face_16', 'z_face_17', 'z_face_18', 'z_face_19', 'z_face_20', 'z_face_21', 'z_face_22', 'z_face_23', 'z_face_24', 'z_face_25', 'z_face_26', 'z_face_27', 'z_face_28', 'z_face_29', 'z_face_30', 'z_face_31', 'z_face_32', 'z_face_33', 'z_face_34', 'z_face_35', 'z_face_36', 'z_face_37', 'z_face_38', 'z_face_39', 'z_face_40', 'z_face_41', 'z_face_42', 'z_face_43', 'z_face_44', 'z_face_45', 'z_face_46', 'z_face_47', 'z_face_48', 'z_face_49', 'z_face_50', 'z_face_51', 'z_face_52', 'z_face_53', 'z_face_54', 'z_face_55', 'z_face_56', 'z_face_57', 'z_face_58', 'z_face_59', 'z_face_60', 'z_face_61', 'z_face_62', 'z_face_63', 'z_face_64', 'z_face_65', 'z_face_66', 'z_face_67', 'z_face_68', 'z_face_69', 'z_face_70', 'z_face_71', 'z_face_72', 'z_face_73', 'z_face_74', 'z_face_75', 'z_face_76', 'z_face_77', 'z_face_78', 'z_face_79', 'z_face_80', 'z_face_81', 'z_face_82', 'z_face_83', 'z_face_84', 'z_face_85', 'z_face_86', 'z_face_87', 'z_face_88', 'z_face_89', 'z_face_90', 'z_face_91', 'z_face_92', 'z_face_93', 'z_face_94', 'z_face_95', 'z_face_96', 'z_face_97', 'z_face_98', 'z_face_99', 'z_face_100', 'z_face_101', 'z_face_102', 'z_face_103', 'z_face_104', 'z_face_105', 'z_face_106', 'z_face_107', 'z_face_108', 'z_face_109', 'z_face_110', 'z_face_111', 'z_face_112', 'z_face_113', 'z_face_114', 'z_face_115', 'z_face_116', 'z_face_117', 'z_face_118', 'z_face_119', 'z_face_120', 'z_face_121', 'z_face_122', 'z_face_123', 'z_face_124', 'z_face_125', 'z_face_126', 'z_face_127', 'z_face_128', 'z_face_129', 'z_face_130', 'z_face_131', 'z_face_132', 'z_face_133', 'z_face_134', 'z_face_135', 'z_face_136', 'z_face_137', 'z_face_138', 'z_face_139', 'z_face_140', 'z_face_141', 'z_face_142', 'z_face_143', 'z_face_144', 'z_face_145', 'z_face_146', 'z_face_147', 'z_face_148', 'z_face_149', 'z_face_150', 'z_face_151', 'z_face_152', 'z_face_153', 'z_face_154', 'z_face_155', 'z_face_156', 'z_face_157', 'z_face_158', 'z_face_159', 'z_face_160', 'z_face_161', 'z_face_162', 'z_face_163', 'z_face_164', 'z_face_165', 'z_face_166', 'z_face_167', 'z_face_168', 'z_face_169', 'z_face_170', 'z_face_171', 'z_face_172', 'z_face_173', 'z_face_174', 'z_face_175', 'z_face_176', 'z_face_177', 'z_face_178', 'z_face_179', 'z_face_180', 'z_face_181', 'z_face_182', 'z_face_183', 'z_face_184', 'z_face_185', 'z_face_186', 'z_face_187', 'z_face_188', 'z_face_189', 'z_face_190', 'z_face_191', 'z_face_192', 'z_face_193', 'z_face_194', 'z_face_195', 'z_face_196', 'z_face_197', 'z_face_198', 'z_face_199', 'z_face_200', 'z_face_201', 'z_face_202', 'z_face_203', 'z_face_204', 'z_face_205', 'z_face_206', 'z_face_207', 'z_face_208', 'z_face_209', 'z_face_210', 'z_face_211', 'z_face_212', 'z_face_213', 'z_face_214', 'z_face_215', 'z_face_216', 'z_face_217', 'z_face_218', 'z_face_219', 'z_face_220', 'z_face_221', 'z_face_222', 'z_face_223', 'z_face_224', 'z_face_225', 'z_face_226', 'z_face_227', 'z_face_228', 'z_face_229', 'z_face_230', 'z_face_231', 'z_face_232', 'z_face_233', 'z_face_234', 'z_face_235', 'z_face_236', 'z_face_237', 'z_face_238', 'z_face_239', 'z_face_240', 'z_face_241', 'z_face_242', 'z_face_243', 'z_face_244', 'z_face_245', 'z_face_246', 'z_face_247', 'z_face_248', 'z_face_249', 'z_face_250', 'z_face_251', 'z_face_252', 'z_face_253', 'z_face_254', 'z_face_255', 'z_face_256', 'z_face_257', 'z_face_258', 'z_face_259', 'z_face_260', 'z_face_261', 'z_face_262', 'z_face_263', 'z_face_264', 'z_face_265', 'z_face_266', 'z_face_267', 'z_face_268', 'z_face_269', 'z_face_270', 'z_face_271', 'z_face_272', 'z_face_273', 'z_face_274', 'z_face_275', 'z_face_276', 'z_face_277', 'z_face_278', 'z_face_279', 'z_face_280', 'z_face_281', 'z_face_282', 'z_face_283', 'z_face_284', 'z_face_285', 'z_face_286', 'z_face_287', 'z_face_288', 'z_face_289', 'z_face_290', 'z_face_291', 'z_face_292', 'z_face_293', 'z_face_294', 'z_face_295', 'z_face_296', 'z_face_297', 'z_face_298', 'z_face_299', 'z_face_300', 'z_face_301', 'z_face_302', 'z_face_303', 'z_face_304', 'z_face_305', 'z_face_306', 'z_face_307', 'z_face_308', 'z_face_309', 'z_face_310', 'z_face_311', 'z_face_312', 'z_face_313', 'z_face_314', 'z_face_315', 'z_face_316', 'z_face_317', 'z_face_318', 'z_face_319', 'z_face_320', 'z_face_321', 'z_face_322', 'z_face_323', 'z_face_324', 'z_face_325', 'z_face_326', 'z_face_327', 'z_face_328', 'z_face_329', 'z_face_330', 'z_face_331', 'z_face_332', 'z_face_333', 'z_face_334', 'z_face_335', 'z_face_336', 'z_face_337', 'z_face_338', 'z_face_339', 'z_face_340', 'z_face_341', 'z_face_342', 'z_face_343', 'z_face_344', 'z_face_345', 'z_face_346', 'z_face_347', 'z_face_348', 'z_face_349', 'z_face_350', 'z_face_351', 'z_face_352', 'z_face_353', 'z_face_354', 'z_face_355', 'z_face_356', 'z_face_357', 'z_face_358', 'z_face_359', 'z_face_360', 'z_face_361', 'z_face_362', 'z_face_363', 'z_face_364', 'z_face_365', 'z_face_366', 'z_face_367', 'z_face_368', 'z_face_369', 'z_face_370', 'z_face_371', 'z_face_372', 'z_face_373', 'z_face_374', 'z_face_375', 'z_face_376', 'z_face_377', 'z_face_378', 'z_face_379', 'z_face_380', 'z_face_381', 'z_face_382', 'z_face_383', 'z_face_384', 'z_face_385', 'z_face_386', 'z_face_387', 'z_face_388', 'z_face_389', 'z_face_390', 'z_face_391', 'z_face_392', 'z_face_393', 'z_face_394', 'z_face_395', 'z_face_396', 'z_face_397', 'z_face_398', 'z_face_399', 'z_face_400', 'z_face_401', 'z_face_402', 'z_face_403', 'z_face_404', 'z_face_405', 'z_face_406', 'z_face_407', 'z_face_408', 'z_face_409', 'z_face_410', 'z_face_411', 'z_face_412', 'z_face_413', 'z_face_414', 'z_face_415', 'z_face_416', 'z_face_417', 'z_face_418', 'z_face_419', 'z_face_420', 'z_face_421', 'z_face_422', 'z_face_423', 'z_face_424', 'z_face_425', 'z_face_426', 'z_face_427', 'z_face_428', 'z_face_429', 'z_face_430', 'z_face_431', 'z_face_432', 'z_face_433', 'z_face_434', 'z_face_435', 'z_face_436', 'z_face_437', 'z_face_438', 'z_face_439', 'z_face_440', 'z_face_441', 'z_face_442', 'z_face_443', 'z_face_444', 'z_face_445', 'z_face_446', 'z_face_447', 'z_face_448', 'z_face_449', 'z_face_450', 'z_face_451', 'z_face_452', 'z_face_453', 'z_face_454', 'z_face_455', 'z_face_456', 'z_face_457', 'z_face_458', 'z_face_459', 'z_face_460', 'z_face_461', 'z_face_462', 'z_face_463', 'z_face_464', 'z_face_465', 'z_face_466', 'z_face_467', 'z_left_hand_0', 'z_left_hand_1', 'z_left_hand_2', 'z_left_hand_3', 'z_left_hand_4', 'z_left_hand_5', 'z_left_hand_6', 'z_left_hand_7', 'z_left_hand_8', 'z_left_hand_9', 'z_left_hand_10', 'z_left_hand_11', 'z_left_hand_12', 'z_left_hand_13', 'z_left_hand_14', 'z_left_hand_15', 'z_left_hand_16', 'z_left_hand_17', 'z_left_hand_18', 'z_left_hand_19', 'z_left_hand_20', 'z_pose_0', 'z_pose_1', 'z_pose_2', 'z_pose_3', 'z_pose_4', 'z_pose_5', 'z_pose_6', 'z_pose_7', 'z_pose_8', 'z_pose_9', 'z_pose_10', 'z_pose_11', 'z_pose_12', 'z_pose_13', 'z_pose_14', 'z_pose_15', 'z_pose_16', 'z_pose_17', 'z_pose_18', 'z_pose_19', 'z_pose_20', 'z_pose_21', 'z_pose_22', 'z_pose_23', 'z_pose_24', 'z_pose_25', 'z_pose_26', 'z_pose_27', 'z_pose_28', 'z_pose_29', 'z_pose_30', 'z_pose_31', 'z_pose_32', 'z_right_hand_0', 'z_right_hand_1', 'z_right_hand_2', 'z_right_hand_3', 'z_right_hand_4', 'z_right_hand_5', 'z_right_hand_6', 'z_right_hand_7', 'z_right_hand_8', 'z_right_hand_9', 'z_right_hand_10', 'z_right_hand_11', 'z_right_hand_12', 'z_right_hand_13', 'z_right_hand_14', 'z_right_hand_15', 'z_right_hand_16', 'z_right_hand_17', 'z_right_hand_18', 'z_right_hand_19', 'z_right_hand_20']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edf11726-a294-4f3e-bef5-89533c0a0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.preprocess = Preprocess()\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.transpose(tf.reshape(inputs, (-1,3,543)), (0,2,1))\n",
    "        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros((1, 543, 3),dtype=tf.float32), lambda: tf.identity(x))\n",
    "        x = self.preprocess(x)\n",
    "        x = self.model(x)[0]\n",
    "        x = x - 1\n",
    "        idxs = tf.where((0<=x) & (x<=58))[...,0]\n",
    "        x = tf.gather(x, idxs)\n",
    "        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int32), lambda: tf.identity(x))\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16934c40-40d7-4bde-aada-bf41dbdfb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [get_model() for _ in MODEL_PATH]\n",
    "for model, path in zip(model_list, MODEL_PATH):\n",
    "    model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f406200-1393-44b1-bce5-f1359258e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf79d8-0c06-4713-bf21-3f95f49f5b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b88a51d7-5bf4-4c6e-86b8-050b928b5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflitemodel_base = TFLiteModel(EnsembleCTCAttentionJointGreedyDecoder(model_list[:3], ctc_weight=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e7ad9b3-4adb-4e2f-aae8-8d556cf28902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cond/Identity:0\", shape=(None, 543, 3), dtype=float32)\n",
      "\n",
      "----\n",
      " (None, 543, 3)\n",
      "INSIDE FILTER (None, 543, 3)\n",
      "Tensor(\"preprocess/SelectV2_5:0\", shape=(None, None, 1629), dtype=float32)\n",
      "\n",
      " Log_psi: Tensor(\"ensemble_ctc_attention_joint_greedy_decoder_2/while/TensorScatterUpdate_2:0\", shape=(1, 62), dtype=float32) \n",
      "new_States_n: Tensor(\"ensemble_ctc_attention_joint_greedy_decoder_2/while/scan/TensorArrayV2Stack/TensorListStack:0\", shape=(None, 1, 62), dtype=float32) \n",
      "new_states_b: Tensor(\"ensemble_ctc_attention_joint_greedy_decoder_2/while/scan/TensorArrayV2Stack_1/TensorListStack:0\", shape=(None, 1, 62), dtype=float32)\n",
      "PREDICTIONS[CTC_Decoder]:\n",
      " Tensor(\"ensemble_ctc_attention_joint_greedy_decoder_2/while:3\", shape=(1, None), dtype=int32) \n",
      "---------------------\n",
      "\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpalxc85q3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpalxc85q3\\assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter._experimental_default_to_single_batch_in_tensor_list_ops = True\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050e4ee-876e-45f7-9785-6bff927aaf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282603b4-2870-4d0a-928c-4157a06d932d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b81b3d7f-2626-47f6-9b0d-01896d706070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = prediction_fn(inputs=all_landmarks_tensor_reshaped)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d8e48-cb23-43ec-aeec-24e5d49eea19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "94a4f51e-02b4-4107-a98b-7921bec74da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "# pred = tflitemodel_base(frames)[\"outputs\"].numpy().argmax(-1)\n",
    "# ''.join([rev_character_map[x] for x in pred]), phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54ae56be-0247-4727-9625-289dd88718e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a0a028d7-f4f0-4e10-adab-6ebbc7fcb1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.sanarane.com'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0f2aafdd-204d-4fde-b72b-1063375d9916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "362cb1be-d5ae-49de-859e-0efa0e09bb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.sanarane.com'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "output = prediction_fn(inputs=all_landmarks_tensor_reshaped)\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "prediction_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde54ef9-8257-4f6f-9b28-00573785205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622e4a6-032e-432b-94ac-069b721345d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
