{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "frame_counter = 0\n",
    "\n",
    "all_landmarks_list = []\n",
    "\n",
    "# Initialize drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "while capture.isOpened() and frame_counter < 100:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Use holistic model to detect landmarks\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Convert back to BGR for rendering\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,255), thickness=1, circle_radius=1),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,255), thickness=1, circle_radius=1)\n",
    "    )\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, \n",
    "        results.right_hand_landmarks, \n",
    "        mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, \n",
    "        results.left_hand_landmarks, \n",
    "        mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    # Display the resulting image with landmarks\n",
    "    cv2.imshow('Holistic Model Landmarks', image)\n",
    "\n",
    "    all_landmarks = []\n",
    "\n",
    "    # Extract pose landmarks\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = [[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]\n",
    "        all_landmarks.extend(pose_landmarks)\n",
    "\n",
    "    # Extract face landmarks\n",
    "    if results.face_landmarks:\n",
    "        face_landmarks = [[lm.x, lm.y, lm.z] for lm in results.face_landmarks.landmark]\n",
    "        all_landmarks.extend(face_landmarks)\n",
    "\n",
    "    # Extract left hand landmarks\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]\n",
    "        all_landmarks.extend(left_hand_landmarks)\n",
    "\n",
    "    # Extract right hand landmarks\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_landmarks = [[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]\n",
    "        all_landmarks.extend(right_hand_landmarks)\n",
    "\n",
    "    # Append the landmarks of this frame to the list\n",
    "    all_landmarks_list.append(all_landmarks)\n",
    "\n",
    "    frame_counter += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all landmarks tensor before reshaping: (100, 543, 3)\n",
      "Shape of all landmarks tensor after reshaping: (100, 1629)\n"
     ]
    }
   ],
   "source": [
    "# Define the expected order of landmarks\n",
    "expected_landmark_order = []\n",
    "\n",
    "# Add face landmarks (assuming 468 landmarks)\n",
    "for i in range(468):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add right hand landmarks (assuming 21 landmarks)\n",
    "for i in range(468, 468 + 21):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add left hand landmarks (assuming 21 landmarks)\n",
    "for i in range(468 + 21, 468 + 21 + 21):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Add pose landmarks (assuming 33 landmarks)\n",
    "for i in range(468 + 21 + 21, 468 + 21 + 21 + 33):\n",
    "    expected_landmark_order.append(i)\n",
    "\n",
    "# Find the maximum number of landmarks\n",
    "max_landmarks = max(len(landmarks) for landmarks in all_landmarks_list)\n",
    "# Ensure that the shape is (100, 543, 3) by padding with NaN values\n",
    "padded_landmarks = []\n",
    "for landmarks in all_landmarks_list:\n",
    "    padded_landmarks.append(landmarks + [[np.nan, np.nan, np.nan]] * (543 - len(landmarks)))\n",
    "\n",
    "# Convert the list of landmarks to a TensorFlow tensor\n",
    "all_landmarks_tensor = tf.convert_to_tensor(padded_landmarks, dtype=tf.float32)\n",
    "\n",
    "print(\"Shape of all landmarks tensor before reshaping:\", all_landmarks_tensor.shape)\n",
    "\n",
    "# Reshape the tensor to have shape (100, 1629)\n",
    "all_landmarks_tensor_reshaped = tf.reshape(all_landmarks_tensor, (100, -1))\n",
    "\n",
    "print(\"Shape of all landmarks tensor after reshaping:\", all_landmarks_tensor_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold\n",
    "import gc\n",
    "# !pip install ipywidgets\n",
    "from tqdm.auto import tqdm\n",
    "import Levenshtein\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_len=MAX_LEN, target_len=64, dim=192, dtype='float32'):\n",
    "    ################# ENCODER #################\n",
    "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
    "#     x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp1)\n",
    "    x = inp1\n",
    "    ksize = 17\n",
    "    drop_rate = 0.2\n",
    "    x = tf.keras.layers.Dense(dim,use_bias=False,name='stem_conv')(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=0,strides=2)(x) #drop_rate=0 since we don't want to drop the whole output\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
    "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "\n",
    "    encoder = tf.keras.Model(inp1,x,name='encoder')\n",
    "\n",
    "    ################# CTC DECDODER #################\n",
    "    inp3 = tf.keras.Input((x.shape[1],dim),name='ctc_decoder_inp2',dtype=dtype)\n",
    "    x = inp3\n",
    "    x = tf.keras.layers.RNN(tf.keras.layers.GRUCell(dim), return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dense(dim*2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='ctc_classifier')(x) #include sos, eos token\n",
    "    ctc_decoder = tf.keras.Model(inp3,x,name='ctc_decoder')\n",
    "\n",
    "    ################# ATT DECODER #################\n",
    "    inp2 = tf.keras.Input((None,),name='att_decoder_inp1',dtype='int32')\n",
    "    inp3 = tf.keras.Input((x.shape[1],dim),name='att_decoder_inp2',dtype=dtype)\n",
    "\n",
    "    x = inp3\n",
    "#     y = tf.keras.layers.Masking(mask_value=0,input_shape=(None,),name='att_decoder_input_masking')(inp2)\n",
    "    y = inp2\n",
    "    y = tf.keras.layers.Embedding(NUM_CLASSES,dim,name='att_decoder_token_emb')(y) #include sos token\n",
    "    y = PosEmbedding(dim,max_len=target_len,name='att_decoder_pos_emb')(y)\n",
    "    y = TransformerDecoderBlock(dim,expand=2,num_heads=4,attn_dropout=0.2,name='att_decoder_block1')(y,x,x)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(NUM_CLASSES,name='att_decoder_classifier')(y)\n",
    "\n",
    "    decoder = tf.keras.Model([inp2,inp3],y,name='att_decoder')\n",
    "\n",
    "    ################### MODEL #####################\n",
    "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
    "    inp2 = tf.keras.Input((None,),dtype='int32')\n",
    "\n",
    "    x = inp1\n",
    "    enc_out = encoder(x)\n",
    "    y = inp2\n",
    "    dec_out = decoder([y, enc_out])\n",
    "    ctc_out = ctc_decoder(enc_out)\n",
    "    model = tf.keras.Model([inp1,inp2], [dec_out,ctc_out])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
